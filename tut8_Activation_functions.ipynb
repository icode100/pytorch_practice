{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM41fo9ZcJss/efSCfgSnh2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# Activation Functions\n","---\n","* ### Step Function\n","\n","$f(x) = \\begin{Bmatrix} 1 & \\text{ if }x\\geq\\theta \\\\ 0 & \\text{ otherwise}\\end{Bmatrix}$\n","\n","* ### sigmoid\n","$f(x) = \\frac{1}{1+e^{-x}}$\n","\n","\n","* ### tanh\n","$f(x) = \\frac{2}{1+e^{-2x}}-1$ this gives value between -1 and +1 hence a good function for score calulation rather than probability calculations. MOstly used in RNNs.\n","\n","* ### ReLU Function\n","$f(x) = max(0,x)$\n","\n","* ### Leaky ReLU Function: This tries to solve vanishing gradients problem\n","$f(x) = \\begin{Bmatrix} x & \\text{ if }x\\geq0 \\\\ a.x & \\text{ otherwise} \\end{Bmatrix}$\n","\n","* ### Softmax\n","$s(y_i) = \\frac{e^{y_i}}{\\sum{e^{y_j}}}$"],"metadata":{"id":"qMIlOTPlnq_8"}},{"cell_type":"code","source":["# output = w*x + b\n","# output = activation_function(output)\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","x = torch.tensor([-1.0, 1.0, 2.0, 3.0])\n","\n","# sofmax\n","output = torch.softmax(x, dim=0)\n","print(output)\n","sm = nn.Softmax(dim=0)\n","output = sm(x)\n","print(output)\n","\n","# sigmoid\n","output = torch.sigmoid(x)\n","print(output)\n","s = nn.Sigmoid()\n","output = s(x)\n","print(output)\n","\n","#tanh\n","output = torch.tanh(x)\n","print(output)\n","t = nn.Tanh()\n","output = t(x)\n","print(output)\n","\n","# relu\n","output = torch.relu(x)\n","print(output)\n","relu = nn.ReLU()\n","output = relu(x)\n","print(output)\n","\n","# leaky relu\n","output = F.leaky_relu(x)\n","print(output)\n","lrelu = nn.LeakyReLU()\n","output = lrelu(x)\n","print(output)\n","\n","#nn.ReLU() creates an nn.Module which you can add e.g. to an nn.Sequential model.\n","#torch.relu on the other side is just the functional API call to the relu function,\n","#so that you can add it e.g. in your forward method yourself.\n","\n","# option 1 (create nn modules)\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(NeuralNet, self).__init__()\n","        self.linear1 = nn.Linear(input_size, hidden_size)\n","        self.relu = nn.ReLU()\n","        self.linear2 = nn.Linear(hidden_size, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        out = self.linear1(x)\n","        out = self.relu(out)\n","        out = self.linear2(out)\n","        out = self.sigmoid(out)\n","        return out\n","\n","# option 2 (use activation functions directly in forward pass)\n","class NeuralNet(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super(NeuralNet, self).__init__()\n","        self.linear1 = nn.Linear(input_size, hidden_size)\n","        self.linear2 = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x):\n","        out = torch.relu(self.linear1(x))\n","        out = torch.sigmoid(self.linear2(out))\n","        return out"],"metadata":{"id":"xcFbwpOrr03X"},"execution_count":null,"outputs":[]}]}