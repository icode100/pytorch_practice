{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPG8evauiPK8vzAoYlJxvRd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# AutoGrad\n","we use it to compute gradients of our weight vector during optimizations. we use autodiff"],"metadata":{"id":"DTXIl_6ZzdT7"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"2DGBgNHFzZ7O","executionInfo":{"status":"ok","timestamp":1709354327761,"user_tz":-330,"elapsed":8189,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}}},"outputs":[],"source":["import torch"]},{"cell_type":"code","source":["x = torch.rand(3,requires_grad=True)"],"metadata":{"id":"yzsW68g2zxQH","executionInfo":{"status":"ok","timestamp":1709354327762,"user_tz":-330,"elapsed":17,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TQ1nF7p0z2a6","executionInfo":{"status":"ok","timestamp":1709354327763,"user_tz":-330,"elapsed":17,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"7fd98b0a-0412-4e1f-a0c5-13cc3ec880ba"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0220, 0.2922, 0.1679], requires_grad=True)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["y = x+2\n","y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ng07jLi5z6o7","executionInfo":{"status":"ok","timestamp":1709354327763,"user_tz":-330,"elapsed":15,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"40f1751d-9e60-42cb-a5ad-1f512a7fe5ad"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2.0220, 2.2922, 2.1679], grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["z = y*y*2\n","z"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbJfDJWL0qKv","executionInfo":{"status":"ok","timestamp":1709354327763,"user_tz":-330,"elapsed":14,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"7f29f467-ea3a-4c85-dd76-716496e470da"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 8.1770, 10.5088,  9.3998], grad_fn=<MulBackward0>)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["z = z.mean()\n","z"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1X-CkVd-0sDn","executionInfo":{"status":"ok","timestamp":1709354327763,"user_tz":-330,"elapsed":11,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"8174af07-aef0-45c3-bea8-acacefd2464a"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(9.3619, grad_fn=<MeanBackward0>)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["z.backward() #dz/dx"],"metadata":{"id":"m87QO2vQ0wQJ","executionInfo":{"status":"ok","timestamp":1709354327763,"user_tz":-330,"elapsed":8,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["x.grad #this is the grandient for the tensor x that is calculated using reverse mode autodiff"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0xC0Y-L1Hjc","executionInfo":{"status":"ok","timestamp":1709354327764,"user_tz":-330,"elapsed":9,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"686f3221-a214-4823-c748-e9b2c728aec8"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2.6960, 3.0563, 2.8906])"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["In the background it does a vector jacobian product. Thus for non scalar vector we cannot doe this because there we need to perform a chain rule as we know. Thus from z on removing mean method we need to pass a gradient vector `v` that we show here."],"metadata":{"id":"RtbJPxCw1hqB"}},{"cell_type":"code","source":["x = torch.rand(3,requires_grad=True)\n","y = x+2\n","z = 2*(y**2)"],"metadata":{"id":"yaeqOj2C1N6s","executionInfo":{"status":"ok","timestamp":1709354369842,"user_tz":-330,"elapsed":415,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["z"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_x-VLpkidHv2","executionInfo":{"status":"ok","timestamp":1709354372706,"user_tz":-330,"elapsed":3,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"1bb12e7f-4027-4515-a32c-564adab5218d"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 8.1038, 14.0323,  9.8810], grad_fn=<MulBackward0>)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["z.backward()# this gives error"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"doiFp8nPdcpT","executionInfo":{"status":"error","timestamp":1709356511259,"user_tz":-330,"elapsed":997,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"c165f96b-cb23-4ed1-97f5-cc1f3adfad66"},"execution_count":43,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"grad can be implicitly created only for scalar outputs","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-81efcb487ece>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# this gives error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                     raise RuntimeError(\n\u001b[0m\u001b[1;32m    118\u001b[0m                         \u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                     )\n","\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"]}]},{"cell_type":"code","source":["z.backward(torch.tensor([0.1,1.0,0.001],dtype = torch.float32))"],"metadata":{"id":"BHHpwJu4dixp","executionInfo":{"status":"ok","timestamp":1709354497836,"user_tz":-330,"elapsed":449,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["print(x.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p4sQ4VCAdfsc","executionInfo":{"status":"ok","timestamp":1709354531648,"user_tz":-330,"elapsed":388,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"844d5079-c5d6-48cf-f406-b7755a59e1f6"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([8.0517e-01, 1.0595e+01, 8.8909e-03])\n"]}]},{"cell_type":"markdown","source":["preventing torch from tracking the gradient history"],"metadata":{"id":"WhKXrS87eT1b"}},{"cell_type":"code","source":["x = torch.randn(3,requires_grad=True)\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LB09rtMWek2s","executionInfo":{"status":"ok","timestamp":1709354784043,"user_tz":-330,"elapsed":401,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"5247c2f9-8a95-4bf0-c11b-65de98869fc9"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-0.3003, -0.1851,  1.6188], requires_grad=True)"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["# using x.requires_grad_(False)\n","x.requires_grad_(False)\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FZ22wOC3eAtl","executionInfo":{"status":"ok","timestamp":1709354786038,"user_tz":-330,"elapsed":16,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"68b72115-3d47-467f-bedc-8ad51cad4571"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-0.3003, -0.1851,  1.6188])\n"]}]},{"cell_type":"code","source":["#using detach function\n","print(x.detach())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GG4R1UGIeyQq","executionInfo":{"status":"ok","timestamp":1709354787107,"user_tz":-330,"elapsed":5,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"aee0e102-70c7-40f6-d455-6b045423e3c6"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-0.3003, -0.1851,  1.6188])\n"]}]},{"cell_type":"code","source":["# using torch.no_grad()\n","with torch.no_grad():\n","    y = x+2\n","    print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jHRscMIEe9rM","executionInfo":{"status":"ok","timestamp":1709354844257,"user_tz":-330,"elapsed":5,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"a7cadf8b-53d5-464d-9b31-9b0e161c49bb"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1.6997, 1.8149, 3.6188])\n"]}]},{"cell_type":"markdown","source":["whenever we call the backward function the gradient for the tensor is accumulated in the grad attribute of the tensor or the values are summed up."],"metadata":{"id":"Y9v19ApdfUzb"}},{"cell_type":"code","source":["import torch\n","def showSumGrad(epochs):\n","    weights = torch.ones(4,requires_grad=True)\n","    print(weights)\n","    for epoch in range(epochs):\n","        model_output = (weights*3).sum()\n","        model_output.backward()\n","        print(weights.grad)\n","    print('\\n')"],"metadata":{"id":"xiWEQ8LrfPmu","executionInfo":{"status":"ok","timestamp":1709355852926,"user_tz":-330,"elapsed":614,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["showSumGrad(1)\n","showSumGrad(2)\n","showSumGrad(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GHVTMEkZfxDH","executionInfo":{"status":"ok","timestamp":1709355854883,"user_tz":-330,"elapsed":5,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"15984203-4650-4ce3-b659-bd78dd551f05"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 1., 1., 1.], requires_grad=True)\n","tensor([3., 3., 3., 3.])\n","\n","\n","tensor([1., 1., 1., 1.], requires_grad=True)\n","tensor([3., 3., 3., 3.])\n","tensor([6., 6., 6., 6.])\n","\n","\n","tensor([1., 1., 1., 1.], requires_grad=True)\n","tensor([3., 3., 3., 3.])\n","tensor([6., 6., 6., 6.])\n","tensor([9., 9., 9., 9.])\n","\n","\n"]}]},{"cell_type":"markdown","source":["Now to prevent this summing up of the gradient we do,"],"metadata":{"id":"gWtWA3tBjcxm"}},{"cell_type":"code","source":["def showSumGrad(epochs):\n","    weights = torch.ones(4,requires_grad=True)\n","    print(weights)\n","    for epoch in range(epochs):\n","        model_output = (weights*3).sum()\n","        print(model_output)\n","        model_output.backward()\n","        print(weights.grad)\n","        ####### making grad zero after each step ###########\n","        weights.grad.zero_()\n","        ####################################################\n","    print('\\n')"],"metadata":{"id":"tyhp-2v6jGdb","executionInfo":{"status":"ok","timestamp":1709356154002,"user_tz":-330,"elapsed":583,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["showSumGrad(1)\n","showSumGrad(2)\n","showSumGrad(3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QuhXcCyDjsVw","executionInfo":{"status":"ok","timestamp":1709356108126,"user_tz":-330,"elapsed":4,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}},"outputId":"56082471-b6fa-42eb-e42b-18cc62a0a810"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1., 1., 1., 1.], requires_grad=True)\n","tensor(12., grad_fn=<SumBackward0>)\n","tensor([3., 3., 3., 3.])\n","\n","\n","tensor([1., 1., 1., 1.], requires_grad=True)\n","tensor(12., grad_fn=<SumBackward0>)\n","tensor([3., 3., 3., 3.])\n","tensor(12., grad_fn=<SumBackward0>)\n","tensor([3., 3., 3., 3.])\n","\n","\n","tensor([1., 1., 1., 1.], requires_grad=True)\n","tensor(12., grad_fn=<SumBackward0>)\n","tensor([3., 3., 3., 3.])\n","tensor(12., grad_fn=<SumBackward0>)\n","tensor([3., 3., 3., 3.])\n","tensor(12., grad_fn=<SumBackward0>)\n","tensor([3., 3., 3., 3.])\n","\n","\n"]}]},{"cell_type":"markdown","source":["we have to use this method while optimizing nn using optimizers for backpropagation.\n","```python\n","weights = torch.ones(4,requires_grad=True)\n","optimizer = torch.optim.SGD(weights,lr = 0.01)\n","optimizer.step()\n","optimizer.zero_grad() # to make gradient zero after each step so that they do not cumulate\n","```"],"metadata":{"id":"JeOFCBzXkaGH"}},{"cell_type":"code","source":[],"metadata":{"id":"qmtm6WMBk7VK","executionInfo":{"status":"ok","timestamp":1709356518465,"user_tz":-330,"elapsed":464,"user":{"displayName":"Ipsit Das","userId":"05027171651305250694"}}},"execution_count":43,"outputs":[]}]}